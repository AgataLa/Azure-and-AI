{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Hello World of Machine learning - the usage of PySpark ML on kaggle's titanic dataset.\n",
        "\n",
        "\n",
        "## Description\n",
        "The sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.\n",
        "\n",
        "One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n",
        "\n",
        "In this challenge, we ask you to complete the analysis of what sorts of people were likely to survive. In particular, we ask you to apply the tools of machine learning to predict which passengers survived the tragedy.\n",
        "\n",
        "## Goals\n",
        "It is your job to predict if a passenger survived the sinking of the Titanic or not. \n",
        "For each in the test set, you must predict a 0 or 1 value for the variable.\n",
        "\n",
        "### Metric\n",
        "Your score is the percentage of passengers you correctly predict. This is known simply as \"accuracy”.\n",
        "\n",
        "### Submission File Format\n",
        "You should submit a csv file with exactly 418 entries plus a header row. Your submission will show an error if you have extra columns (beyond PassengerId and Survived) or rows.\n",
        "\n",
        "The file should have exactly 2 columns:\n",
        "\n",
        "PassengerId (sorted in any order)\n",
        "Survived (contains your binary predictions: 1 for survived, 0 for deceased)\n",
        "```\n",
        "PassengerId,Survived\n",
        " 892,0\n",
        " 893,1\n",
        " 894,0\n",
        " Etc.\n",
        " ```\n",
        "\n",
        "## Data Overview\n",
        "The data has been split into two groups:\n",
        "- training set (train.csv)\n",
        "- test set (test.csv)\n",
        "\n",
        "\n",
        "The training set should be used to build your machine learning models. For the training set, we provide the outcome (also known as the “ground truth”) for each passenger. Your model will be based on “features” like passengers’ gender and class. You can also use feature engineering to create new features.\n",
        "\n",
        "The test set should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.\n",
        "\n",
        "\n",
        "### Data Dictionary\n",
        "Variable\t| Definition\t| Key\n",
        "--- | --- | ---\n",
        "survival|\tSurvival |\t0 = No, 1 = Yes \n",
        "pclass\t|Ticket class\t| 1 = 1st, 2 = 2nd, 3 = 3rd \n",
        "sex | Sex\t| \n",
        "Age\t| Age in years\t| \n",
        "sibsp\t|# of siblings / spouses aboard the Titanic\t | \n",
        "parch\t|# of parents / children aboard the Titanic\t | \n",
        "ticket\t|Ticket number\t|\n",
        "fare\t|Passenger fare\t|\n",
        "cabin\t|Cabin number\t|\n",
        "embarked|\tPort of Embarkation\t|C = Cherbourg, Q = Queenstown, S = Southampton\n",
        "\n",
        "\n",
        "\n",
        "#### Variable Notes\n",
        "- pclass: A proxy for socio-economic status (SES)\n",
        "* 1st = Upper\n",
        "* 2nd = Middle\n",
        "* 3rd = Lower\n",
        "\n",
        "- age: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n",
        "\n",
        "- sibsp: The dataset defines family relations in this way...\n",
        "- Sibling = brother, sister, stepbrother, stepsister\n",
        "- Spouse = husband, wife (mistresses and fiancés were ignored)\n",
        "\n",
        "- parch: The dataset defines family relations in this way...\n",
        "- Parent = mother, father\n",
        "- Child = daughter, son, stepdaughter, stepson\n",
        "- Some children travelled only with a nanny, therefore parch=0 for them.\n",
        "\n",
        "\n",
        "### Prerequisites:\n",
        "Understanding of following topics;\n",
        "  * Python https://www.kaggle.com/learn/python  \n",
        "  * Machine learning https://www.kaggle.com/learn/machine-learning\n",
        "  * Apache Spark \n",
        "\n",
        "\n",
        "Ref. https://www.kaggle.com/c/titanic"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Importing needful libraries"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "0ca4584e-0ff9-420c-ba0c-2d4531491d57"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession \n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.sql.functions import mean, col, split, count, regexp_extract, when, lit\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "from pyspark.ml.feature import VectorAssembler \n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml.feature import QuantileDiscretizer\n"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "1ec09083-d287-43d0-adc4-2cff06b851f8"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### pyspark.ml.pipeline\n",
        "A simple pipeline, which acts as an estimator. A Pipeline consists of a sequence of stages, each of which is either an Estimator or a Transformer. When Pipeline.fit() is called, the stages are executed in order. If a stage is an Estimator, its Estimator.fit() method will be called on the input dataset to fit a model. Then the model, which is a transformer, will be used to transform the dataset as the input to the next stage. If a stage is a Transformer, its Transformer.transform() method will be called to produce the dataset for the next stage. The fitted model from a Pipeline is a PipelineModel, which consists of fitted models and transformers, corresponding to the pipeline stages. If stages is an empty list, the pipeline acts as an identity transformer.\n",
        "- Estimator: Abstract class for estimators that fit models to data.\n",
        "- Transformer: Abstract class for transformers that transform one dataset into another. \n",
        "\n",
        "### pyspark.sql.functions\n",
        "- mean - is an alias for avg().\n",
        "- col - returns a Column based on the given column name.\n",
        "- split - splits str around pattern (pattern is a regular expression).\n",
        "- count - counts the number of records for each group.\n",
        "- regexp_extract - Extract a specific group matched by a Java regex, from the specified string column. If the regex did not match, or the specified group did not match, an empty string is returned.\n",
        "- when - Evaluates a list of conditions and returns one of multiple possible result expressions. If Column.otherwise() is not invoked, None is returned for unmatched conditions. Parameters: 1. condition – a boolean Column expression. 2. value – a literal value, or a Column expression.\n",
        "- lit - creates a Column of literal value.\n",
        "\n",
        "### pyspark.ml.feature.StringIndexer\n",
        "A label indexer that maps a string column of labels to an ML column of label indices. If the input column is numeric, we cast it to string and index the string values. The indices are in [0, numLabels). By default, this is ordered by label frequencies so the most frequent label gets index 0. The ordering behavior is controlled by setting stringOrderType. Its default value is ‘frequencyDesc’.\n",
        "\n",
        "### pyspark.ml.feature.VectorAssembler\n",
        "A feature transformer that merges multiple columns into a vector column.\n",
        "\n",
        "### pyspark.ml.evaluation.MulticlassClassificationEvaluator\n",
        "Evaluator for Multiclass Classification, which expects two input columns: prediction and label. Calling evaluate returns metric.\n",
        "\n",
        "### pyspark.ml.feature.QuantileDiscretizer\n",
        "QuantileDiscretizer takes a column with continuous features and outputs a column with binned categorical features. The number of bins can be set using the numBuckets parameter. It is possible that the number of buckets used will be less than this value, for example, if there are too few distinct values of the input to create enough distinct quantiles.\n",
        "\n",
        "NaN handling: Note also that QuantileDiscretizer will raise an error when it finds NaN values in the dataset, but the user can also choose to either keep or remove NaN values within the dataset by setting handleInvalid parameter. If the user chooses to keep NaN values, they will be handled specially and placed into their own bucket, for example, if 4 buckets are used, then non-NaN data will be put into buckets[0-3], but NaNs will be counted in a special bucket[4].\n",
        "\n",
        "Algorithm: The bin ranges are chosen using an approximate algorithm (see the documentation for approxQuantile() for a detailed description). The precision of the approximation can be controlled with the relativeError parameter. The lower and upper bin bounds will be -Infinity and +Infinity, covering all real values.\n",
        "\n",
        "\n",
        "Ref. \n",
        "- https://spark.apache.org/docs/latest/api/python/pyspark.ml.html?highlight=vectorassembler#pyspark.ml.Transformer\n",
        "- https://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "ff9d5af5-d2eb-48e7-9595-1b62e56fbb81"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Beginning with SparkSession"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "690dbd35-28eb-44b9-a4c2-21c6f069582f"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Spark running locally https://spark.apache.org/docs/latest/quick-start.html\n",
        "# Builder, get or create \n",
        "# Scala -> dataset\n",
        "# Python -> dataframe"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "1842bae3-88cc-4f53-83f1-90cdb8a959b7"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### The entry point into all functionality in Spark is the SparkSession class.\n",
        "###### To create a basic SparkSession, just use SparkSession.builder\n",
        "\n",
        "Before 2.0, the entry point to Spark Core was `sparkContext`. In previous versions of Spark, you had to create a SparkConf and SparkContext to interact with Spark.\n",
        "\n",
        "For Scala:\n",
        "\n",
        "Past, example:\n",
        "```scala\n",
        "//set up the spark configuration and create contexts\n",
        "val sparkConf = new SparkConf().setAppName(\"Spark ML example on titanic data \").setMaster(\"local\")\n",
        "// your handle to SparkContext to access other context like SQLContext\n",
        "val sc = new SparkContext(sparkConf).set(\"spark.some.config.option\", \"some-value\")\n",
        "val sqlContext = new org.apache.spark.sql.SQLContext(sc)\n",
        "```\n",
        "\n",
        "Now, no need to create explicitly SparkConf, SparkContext or SQLContext, as they’re encapsulated within the SparkSession. \n",
        "```scala\n",
        "val spark = SparkSession.builder()\n",
        ".master(\"local\")\n",
        ".appName(\"Spark ML example on titanic data \")\n",
        ".config(\"spark.some.config.option\", \"some-value\")\n",
        ".getOrCreate()\n",
        "SparkSession.builder()\n",
        "```"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "e4d212dc-40ae-4d7e-888a-99cbfbdb1bb4"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"Spark ML example on titanic data \") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# FYI - it's required to run spark locally. Synapse/Databricks creates SparkSession on notebook creation/opening "
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "f6ef0f29-14a5-4b30-87a0-b179de499529"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STORY #1 - load\n",
        "\n",
        "[PL] \n",
        "\n",
        "1. Wczytać dane\n",
        "2. Wyświetlić je w najprostszy sposób\n",
        "3. Policzyć, dla ilu pasażerów mamy dane"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "d30533a3-476b-4545-be81-024cae08ed3f"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Next, we have to import the dataset. \n"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "c130ce0e-f173-44d3-b1e6-2bfc4365605c"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* inferSchema (default false): infers the input schema automatically from data. It requires one extra pass over the data. \n",
        "\n",
        "Ref. https://spark.apache.org/docs/2.0.2/api/java/org/apache/spark/sql/DataFrameReader.html"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "824ef2d7-d61c-4988-8210-ed4ffc59895c"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "titanic_df = spark.read.load('abfss://synapseekot@synapseekot.dfs.core.windows.net/titanic/train.csv', format='csv'\n",
        ", header=True,\n",
        ",inferSchema=True\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "94908dfd-c2c4-4cd2-a2d2-56ab915bf95c"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* display function - The easiest way to create a DataFrame visualization in Synapse/Databricks is to call display(<dataframe-name>). For example, if you have a Spark DataFrame diamonds_df of a diamonds dataset grouped by diamond color, computing the average price, and you call"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "df697c8a-3d0c-4558-b755-78ed493a6145"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO Print/display dataframe\n",
        "# <***>(titanic_df)"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "b519ceb7-2bf3-49fc-ae05-67d2927c3f0d"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* .printSchema() - If you want to see the Structure (Schema) of the DataFrame, then use the following command."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "159cee5a-1388-41d1-98c7-e14e494ef608"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO Print/display dataframe\n",
        "# titanic_df.<***>"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "6a719dd4-5537-4f48-a104-117e31d4aa5d"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO Count total number of passengers\n",
        "\n",
        "passengers_count: int = 0  #= titanic_df.<***>"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "64e3530e-c3f9-4662-92cc-334864bba175"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(passengers_count)"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "043bb2eb-6be7-4433-8b44-8e09a4374596"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STORY #2 - explore\n",
        "\n",
        "[PL]\n",
        "1. Wyświetl 5 pierwszych wierszy\n",
        "2. Wyświetl statystyki dla danych\n",
        "3. Wyświetl kolumny: \"Survived\",\"Pclass\",\"Embarked\" (tylko te)"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "bbb137fa-e351-4aec-95fe-4a370e436311"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Viewing few rows"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "30fd15a0-6ba1-43da-a0d3-6af2ff06e30a"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Show first 5 rows of data\n",
        "\n",
        "titanic_df.show(#)"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "bdb15ab6-c209-43ae-9fed-437a550460e2"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary of data"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "316f082c-97d5-40cc-b4b2-3cfddbf9ff87"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* describe: The function describe returns a DataFrame containing information such as number of non-null entries (count), mean, standard deviation, and minimum and maximum value for each numerical column."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "e186c503-402a-4128-a9f0-664414e3684f"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO Describe dataset and show it. Result should returns a DataFrame containing information such as number of non-null entries (count), mean, standard deviation, and minimum and maximum value for each numerical column.\n",
        "\n",
        "# titanic_df.<***>.<***>"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "1519a4c1-5cf1-4ab3-ab0c-f72e31599888"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### selecting few features"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "a542b532-7cd6-488b-90e6-96bbdca5903f"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO show features for columns: \"Survived\",\"Pclass\",\"Embarked\"\n",
        "\n",
        "# titanic_df.<***>.show()"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "1389c002-93f5-4699-897c-f5a2f6414adb"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STORY #3 - count survivors\n",
        "\n",
        "[PL] \n",
        "\n",
        "1. Policz ilu pasażerów przeżyło i wyświetl w tabeli\n",
        "2. Stwórz dataframe z informacją z p.1\n",
        "3. Wyświetl stworzony dataframe"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "76a05af9-aa9d-44f8-92a1-23804f1739ab"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Let's do some simple exploratory data analysis (EDA)"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "744086f2-4fa6-4dc5-9272-39b60edeefa3"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Knowing the number of Passengers Survived ?"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "60e28ece-d544-490f-baca-e57197a9acbe"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO show how many passengers survived and how many not, d\n",
        "\n",
        "# All methods https://spark.apache.org/docs/1.6.1/api/java/org/apache/spark/sql/DataFrame.html\n",
        "\n",
        "# titanic_df.<***>.<***>.show()\n"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "adcd1a34-fab8-47b6-b32c-526d6d784dc1"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "All methods https://spark.apache.org/docs/1.6.1/api/java/org/apache/spark/sql/DataFrame.html"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "e8ebc84b-5197-4e2b-8ba5-42a50a04affe"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO present as (donut) the percentage of how many passengers survived and how many not\n",
        "\n",
        "# output = titanic_df.<***>.<***>\n",
        "\n",
        "# display(output)"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "a74fc92b-3ce6-453a-b4cc-0c618c898865"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Complete the sentance:\n",
        "\n",
        "out of ... passengers in dataset, only about ... survived."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "9ad01c33-2062-44d5-a2a3-f119f7345f79"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STORY #4 - Do gender & ticket class is correlated somehow with the number of survivors?"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "ed6212d9-e105-4db1-a1e8-c25a591ac742"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### To know the particulars about survivors we have to explore more of the data.\n",
        "###### The survival rate can be determined by different features of the dataset such as Sex, Port of Embarcation, Age; few to be mentioned."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "7f8e46a2-d598-473b-91b5-99c30a5ab40d"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking survival rate using feature Sex"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "abafcfc2-5bb1-4074-961a-6f4fcedd6684"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO show in a table how many male/female passengers survived\n",
        "\n",
        "# titanic_df.<***>.count().show()"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "93960e1e-0cb7-485a-bace-e63d9d710874"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write down your observations and conclusions. e.g.\n",
        "- Although the number of males are more than females on ship, the female survivors are twice the number of males saved."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "7b1224a0-4f29-4572-940b-0f4d634a908b"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO check if there is any correlation between ticket class and survived passengers number\n",
        "\n",
        "# titanic_df.<***>.count().show()"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "e7cef8c7-bdb4-4a63-91ce-3715d99b8bb7"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write down your observations and conclusions"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "06e29696-c1de-48e4-b812-fd3537631a75"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STORY #5 - data quality\n",
        "\n",
        "[PL]\n",
        "\n",
        "1. Zapoznaj się z napisaną metodą i znajdź w niej błąd\n",
        "2. Wyświetl tabelę z dwoma kolumnami (nazwa kolumny, liczba wystąpień wartości zerowych)"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "4394df37-3f4c-4aa4-852f-baf2ef0e0017"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Checking Null values"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "c13b9507-c15e-4596-b82b-53ffcc81a847"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This function use to print feature with null values and null count \n",
        "def null_value_count(df):\n",
        "  null_columns_counts = []\n",
        "  numRows = df.count()\n",
        "  for k in df.columns:\n",
        "    nullRows = df.where(col(k).isNull()).count()\n",
        "    if(nullRows > 0):\n",
        "      temp = k,nullRows\n",
        "      null_columns_counts.append(temp)\n",
        "  return(null_columns_counts)"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "5e1a4f79-37bf-49eb-a435-c1cad07452cc"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calling function\n",
        "null_columns_count_list = null_value_count(titanic_df)"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "ade0d5bf-b651-4a4b-9c10-45a9db9150f4"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.createDataFrame(null_columns_count_list, ['Column_With_Null_Value', 'Null_Values_Count']).show()"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "3a45d446-0c33-4183-ac6a-335c1871ba3f"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Age feature has XX null values."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "28abee8e-421c-4861-afda-64e62f7b9e7f"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STORY #6 Can we guess the age of people based on another feature?"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "20e186db-0f2c-4bbd-9fa1-9a5cf1eb5dff"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO print mean age of all passengers\n",
        "\n",
        "# mean_age = titanic_df.select(mean(<***>)).collect()[0][0]\n",
        "\n",
        "print(mean_age)"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "d7b3a459-a50c-43ec-9774-3c5af8b7405b"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO show the feature which can give the most valuable information which could be correlated with age\n",
        "\n",
        "# titanic_df.select(<***>).show()"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "aa0136d7-d01d-457f-a0c0-1e99188ff4b6"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* null values represents \"no value\" or \"nothing\", it's not even an empty string or zero. It can be used to represent that nothing useful exists.\n",
        "* NaN stands for \"Not a Number\", it's usually the result of a mathematical operation that doesn't make sense, e.g. 0.0/0.0"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "8d56a573-0aca-4f28-b8d1-4332558b44f4"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To replace these NaN values, we can assign them the mean age of the dataset.But the problem is, there were many people with many different ages. We just cant assign a 4 year kid with the mean age that is 29 years."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "2bade71e-3e96-4ca4-b4fe-062aa9857f5e"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "we can check the Name feature. Looking upon the feature, we can see that the names have a salutation like Mr or Mrs. Thus we can assign the mean values of Mr and Mrs to the respective groups"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "dc478e83-aa33-4e33-a6a4-429300d0cc9f"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "titanic_df = titanic_df.withColumn(\"Initial\",regexp_extract(col(\"Name\"),\"([A-Za-z]+)\\.\",1))\n"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "9b532726-4185-4fe7-9cd1-4971793e3f28"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the Regex \"\"[A-Za-z]+).\" we extract the initials from the Name. It looks for strings which lie between A-Z or a-z and followed by a .(dot)."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "c1156e8e-ec9b-4e1d-8b42-937e13fb0102"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "titanic_df.show()"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "bdc98b05-9cb8-4f61-8b3d-54dbbf94939f"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO show unique initials\n",
        "\n",
        "# titanic_df.select(<***>).<***>.show()\n"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "03dc3560-2403-4986-965f-4f76cfb2541c"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are some misspelled Initials like Mlle or Mme that stand for Miss. I will replace them with Miss and same thing for other values."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "2d75a3f6-de9f-423a-ad9a-7bc174a065a9"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO replace misspelled initials like Mlle with Miss and same thing for other values.\n",
        "# mapping dict\n",
        "'Mlle' -> 'Miss'\n",
        "'Mme' -> 'Miss'\n",
        "'Ms'  -> 'Miss'\n",
        "'Dr' -> 'Mr'\n",
        "'Major' -> 'Mr'\n",
        "'Lady' -> 'Mrs'\n",
        "'Countess' -> 'Mrs'\n",
        "'Jonkheer' -> 'Other'\n",
        "'Col' -> 'Other'\n",
        "'Rev' -> 'Other'\n",
        "'Capt' -> 'Mr'\n",
        "'Sir' -> 'Mr'\n",
        "'Don' -> 'Mr'\n",
        "\n",
        "# titanic_df = titanic_df.<***>([<***>], [<***>])\n"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "60906335-b084-4f42-8cea-d00bc6429346"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO verify if now the unique initials are correct\n",
        "\n",
        "# titanic_df.select(<***>).<***>.show()\n"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "bde7df9e-9cb5-4576-99ac-210351b04895"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO chceck average age per initials\n",
        "\n",
        "# titanic_df.groupby(<***>).avg('Age').collect()"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "d37641c1-d926-4a82-955b-378168fc4763"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's impute missing values in age feature based on average age of Initials"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "0a1113b8-8449-4cef-b024-1895a65390ae"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO replace <***> with correct numbers \n",
        "\n",
        "titanic_df = titanic_df.withColumn(\"Age\",when((titanic_df[\"Initial\"] == \"Miss\") & (titanic_df[\"Age\"].isNull()), <***> ).otherwise(titanic_df[\"Age\"]))\n",
        "titanic_df = titanic_df.withColumn(\"Age\",when((titanic_df[\"Initial\"] == \"Other\") & (titanic_df[\"Age\"].isNull()), <***>).otherwise(titanic_df[\"Age\"]))\n",
        "titanic_df = titanic_df.withColumn(\"Age\",when((titanic_df[\"Initial\"] == \"Master\") & (titanic_df[\"Age\"].isNull()), <***>).otherwise(titanic_df[\"Age\"]))\n",
        "titanic_df = titanic_df.withColumn(\"Age\",when((titanic_df[\"Initial\"] == \"Mr\") & (titanic_df[\"Age\"].isNull()), <***>).otherwise(titanic_df[\"Age\"]))\n",
        "titanic_df = titanic_df.withColumn(\"Age\",when((titanic_df[\"Initial\"] == \"Mrs\") & (titanic_df[\"Age\"].isNull()), <***>).otherwise(titanic_df[\"Age\"]))\n"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "91ef969a-1d86-43d5-94cf-8ce6957da51d"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the imputation"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "5650f952-d396-4707-9e96-0e00f96e3f74"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO \n",
        "# titanic_df.filter(titanic_df.Age== <***> ).select(\"Initial\").show()"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "fe74db8f-7ee1-407b-acbc-543ee23a1b1b"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "titanic_df.select(\"Age\").show()"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "df022e48-e3f0-44c6-810e-13e7192c684a"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STORY #7 - Simplification"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "360533ea-7784-42d1-ab05-b9cfc5458f6f"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Embarked feature has only two missining values. Let's check values within Embarked"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "d07372d3-6be7-44c1-8e78-ed57c43a2279"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO \n",
        "# titanic_df.groupBy(<***>).count().show()"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "9f41e0f9-8abf-4b17-b2cd-0cd0eb95af5c"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Majority Passengers boarded from \"S\". We can impute with \"S\""
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "d8344e8c-015a-462b-9e27-ba6b20b895b1"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "titanic_df = titanic_df.na.fill({\"Embarked\" : 'S'})\n"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "7b5c6b56-6a82-49c2-a5ed-05a7d7645ca2"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can drop Cabin features as it has lots of null values"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "3de33bd1-b4ad-46f8-ad6a-ba71941f5520"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "titanic_df = titanic_df.<***>(<***>)"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "8399694a-8889-488f-93d5-2f43c699a428"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "titanic_df.printSchema()"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "715d33ac-35d2-4027-b6b1-1e5ecfbb4f2b"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STORY #8 - [PL] \"Z rodziną wychodzi się dobrze tylko na zdjęciach\""
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "8a3eeaa0-92cd-4a3d-a38e-a1c584c5e117"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can create a new feature called \"Family_size\" and \"Alone\" and analyse it. This feature is the summation of Parch(parents/children) and SibSp(siblings/spouses). It gives us a combined data so that we can check if survival rate have anything to do with family size of the passengers"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "47abbdac-e124-4bc7-98a4-eeb4cede88c4"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "titanic_df = titanic_df.withColumn(\"Family_Size\",col('SibSp')+col('Parch'))"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "7190540b-77fc-4acd-bdf0-adfa19fbf9f9"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "titanic_df.groupBy(\"Family_Size\").count().show()"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "fb5ce7ff-f057-4ccc-bdd9-b7d85612522d"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "titanic_df = titanic_df.withColumn('Alone',lit(0))\n"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "4cd260ad-4311-4ab5-b1ee-e420a63d1701"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO put proper value\n",
        "\n",
        "# titanic_df = titanic_df.withColumn(\"Alone\",when(titanic_df[\"Family_Size\"] == 0, <***>).otherwise(titanic_df[\"Alone\"]))"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "28dfaf43-3dc6-44c1-b465-187319af0031"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "titanic_df.columns"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "f3c71e6b-d39b-4e36-8c84-410d61152d9b"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STORY #9 - numeric representation"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "4ff6e3c7-a05b-4a30-b75c-83afe4cfe151"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets convert Sex, Embarked & Initial columns from string to number using StringIndexer\n",
        "\n",
        "\n",
        "Helper:\n",
        "* StringIndexer: \n",
        " * converts a single column to an index column (similar to a factor column in R)\n",
        " * Use it if you want the Machine Learning algorithm to identify column as categorical variable or if want to convert the textual data to numeric data keeping the categorical context.\n",
        " * e,g converting days(Monday, Tuesday...) to numeric representation.\n",
        " \n",
        "* VectorIndexer: \n",
        " * is used to index categorical predictors in a featuresCol column. Remember that featuresCol is a single column consisting of vectors (refer to featuresCol and labelCol). Each row is a vector which contains values from each predictors.\n",
        " * if you have string type predictors, you will first need to use index those columns with StringIndexer. featuresCol contains vectors, and vectors does not contain string values.\n",
        " * use this if we do not know the types of data incoming. so we leave the logic of differentiating between categorical and non categorical data to the algorithm using Vector Indexer.\n",
        " * e,g - Data coming from 3rd Party API, where data is hidden and is ingested directly to the training model.\n",
        "\n",
        "\n",
        "Ref.: https://mingchen0919.github.io/learning-apache-spark/StringIndexer-and-VectorIndexer.html"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "b82ebefd-e9f1-4511-af9b-b20a9a894acc"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "indexers = [StringIndexer(inputCol=column, outputCol=column+\"_index\").fit(titanic_df) for column in [\"Sex\",\"Embarked\",\"Initial\"]]\n",
        "pipeline = Pipeline(stages=indexers)\n",
        "titanic_df = pipeline.fit(titanic_df).transform(titanic_df)"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "c8adf723-ab7f-421b-811d-08e3b3f0b284"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "titanic_df.show()"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "264831c1-96f9-47da-ab7e-08fa2c002c21"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "titanic_df.printSchema()"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "4cf65203-4863-4443-83e7-10ccac85fc95"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STORY #10 - cleaning"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "96cf7aa7-d4dc-4b80-b093-c1ed4e91fec6"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Drop columns which are not required"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "457a68fa-84fe-43e0-9068-047ebae4a0dc"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO drop columns \"PassengerId\",\"Name\",\"Ticket\",\"Cabin\",\"Embarked\",\"Sex\",\"Initial\"\n",
        "# titanic_df = titanic_df.drop(<***>)"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "e4d90c86-b727-4fa4-b31d-451c7774867e"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "titanic_df.show()"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "99628455-9237-4775-81c9-0d15be2f9dea"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STORY #11 - \"It's all about vectors!!!!\"\n",
        "\n",
        "[PL] \n",
        "\n",
        "1."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "0d59fb41-5d31-44f7-9c81-f5dc58040b06"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's put all features into vector"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "02a37162-f2d0-4a1d-a8b5-bdbb781ffd0d"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature = VectorAssembler(inputCols=titanic_df.columns[1:],outputCol=\"features\")\n",
        "feature_vector= feature.transform(titanic_df)"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "85f71da6-e289-4b63-aab9-613c7e5b9784"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_vector.show()"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "0bf72e9d-01f5-46e9-9f5a-ca02ad402998"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STORY #12 - test & train datasets *\n",
        "\n",
        "* from main train dataset"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "e4ddd26b-81b7-4bcd-a3b9-5cc03ae520d6"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that the data is all set, let's split it into training and test. I'll be using 80% of it."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "6fd876f9-93c1-4ffe-a7de-8f5c34b48675"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO put proper values\n",
        "\n",
        "# (trainingData, testData) = feature_vector.randomSplit([0.8, <***> ],seed = <***>)"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "aac36bd5-0120-476b-914e-6442361cdb7e"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STORY #13 - MACHINE LEARNING"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "6919d25e-25ed-4862-b9cf-3b16572b74e4"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Modelling"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "294f0db4-c1db-433a-ae74-7f68e4984d6b"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Here is the list of few Classification Algorithms from Spark ML \n",
        "\n",
        "LogisticRegression\n",
        "\n",
        "DecisionTreeClassifier\n",
        "\n",
        "RandomForestClassifier\n",
        "\n",
        "Gradient-boosted tree classifier\n",
        "\n",
        "NaiveBayes\n",
        "\n",
        "Support Vector Machine\n",
        "\n",
        "\n",
        "or any ref. https://spark.apache.org/docs/latest/ml-classification-regression.html"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "630217ea-619c-4f48-bbcf-d6ccdda1a662"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### LogisticRegression\n",
        "\n",
        "\n",
        "\n",
        "PL Intro\n",
        "\n",
        "W naukach biologiczno-medycznych często mamy do czynienia ze zmiennymi typu dychotomicznego,\n",
        "jak np. zmienna Występowanie_Nowotworu (1-tak, 0-nie), czy zmienna Przeżycie (1-tak, 0-nie).\n",
        "Takiej sytuacji istotnym może okazać się pytanie, jakie zmienne istotnie wpływają na przeżycie czy\n",
        "wystąpienie nowotworu. W tego typu zagadnieniach świetnie sprawdza się regresja logistyczna.\n",
        "Analiza i interpretacja wyników regresji logistycznej jest bardzo podobna do metod klasycznej\n",
        "regresji. Najważniejszymi różnicami pomiędzy tymi dwiema metodami są\n",
        "• Bardziej skomplikowane i czasochłonne obliczenia,\n",
        "• Wyliczanie wartości i sporządzanie wykresów reszt zazwyczaj nie wnosi nic nowego do\n",
        "modelu.\n",
        "\n",
        "Założenia regresji logistycznej\n",
        "• Losowy dobór próby;\n",
        "• Odpowiednie kodowanie (model regresji logistycznej wylicza prawdopodobieństwo, że\n",
        "zmienna zależna przyjmuje wartość 1);\n",
        "• Uwzględnienie wszystkich istotnych zmiennych;\n",
        "• Wyłączenie z modelu wszystkich nieistotnych zmiennych;\n",
        "• Zależność transformacji logitowej od zmiennych niezależnych jest liniowa;\n",
        "• Model regresji logistycznej nie wyjaśnia efektów interakcji zmiennych niezależnych;\n",
        "• Zmienne niezależne nie mogą być współliniowe;\n",
        "• Regresja logistyczna jest wrażliwa na występowanie punktów odstających. Przed\n",
        "rozpoczęciem analizy należy je usunąć (wykrycie przypadków odstających umożliwia analiza\n",
        "reszt);\n",
        "• Próba musi być dostatecznie liczna (co najmniej n=100); \n",
        "\n",
        "\n",
        "W regresji logistycznej, oprócz współczynników regresji i ich statystycznej istotności, dochodzi jeszcze\n",
        "dodatkowy parametr: iloraz szans (odds ratio)\n",
        "\n",
        "Ref. http://home.agh.edu.pl/~mmd/_media/dydaktyka/adp/regresja_logistyczna.pdf"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "03d881af-25d6-40f8-b44e-b322cf0976a8"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "lr = LogisticRegression(labelCol=\"Survived\", featuresCol=\"features\")\n",
        "#Training algo\n",
        "lrModel = lr.fit(trainingData)\n",
        "lr_prediction = lrModel.transform(testData)\n",
        "lr_prediction.select(\"prediction\", \"Survived\", \"features\").show()\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"Survived\", predictionCol=\"prediction\", metricName=\"accuracy\")"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "0066283f-2d9a-49bb-b401-14ef310ee117"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Evaluating accuracy of LogisticRegression."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "06214715-7038-4ff7-924c-9ddc61d89114"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr_accuracy = evaluator.evaluate(lr_prediction)\n",
        "print(\"Accuracy of LogisticRegression is = %g\"% (lr_accuracy))\n",
        "print(\"Test Error of LogisticRegression = %g \" % (1.0 - lr_accuracy))"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "caec21f1-5d60-4779-ba42-8b0a10a76f7d"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### DecisionTreeClassifier"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "842848c2-37ae-4fc0-99b6-d6f2a8cc3f5a"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "dt = DecisionTreeClassifier(labelCol=\"Survived\", featuresCol=\"features\")\n",
        "dt_model = dt.fit(trainingData)\n",
        "dt_prediction = dt_model.transform(testData)\n",
        "dt_prediction.select(\"prediction\", \"Survived\", \"features\").show()\n"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "7558c221-4efd-4ef4-af20-d25ad805a8eb"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Evaluating accuracy of DecisionTreeClassifier."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "0e3c4354-581c-4bc8-8219-877756b07fa9"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dt_accuracy = evaluator.evaluate(dt_prediction)\n",
        "print(\"Accuracy of DecisionTreeClassifier is = %g\"% (dt_accuracy))\n",
        "print(\"Test Error of DecisionTreeClassifier = %g \" % (1.0 - dt_accuracy))\n"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "6d5f31bf-fbda-4583-bfdc-5c71bf6f5bbf"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### RandomForestClassifier"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "a44c26b5-1e64-4e56-995d-5ba93f8b0a4b"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "rf = DecisionTreeClassifier(labelCol=\"Survived\", featuresCol=\"features\")\n",
        "rf_model = rf.fit(trainingData)\n",
        "rf_prediction = rf_model.transform(testData)\n",
        "rf_prediction.select(\"prediction\", \"Survived\", \"features\").show()"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "43ef7c50-6cef-4236-a381-cf51af1be197"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Evaluating accuracy of RandomForestClassifier."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "01a761a1-b4ad-4afa-befc-7433c533ba16"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rf_accuracy = evaluator.evaluate(rf_prediction)\n",
        "print(\"Accuracy of RandomForestClassifier is = %g\"% (rf_accuracy))\n",
        "print(\"Test Error of RandomForestClassifier  = %g \" % (1.0 - rf_accuracy))"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "ec9b2769-e6b7-43c8-82eb-c3f377abb5f2"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Gradient-boosted tree classifier"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "4770852e-d6d9-4999-8347-6afe75401794"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import GBTClassifier\n",
        "gbt = GBTClassifier(labelCol=\"Survived\", featuresCol=\"features\",maxIter=10)\n",
        "gbt_model = gbt.fit(trainingData)\n",
        "gbt_prediction = gbt_model.transform(testData)\n",
        "gbt_prediction.select(\"prediction\", \"Survived\", \"features\").show()\n"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "825e640c-b015-4ec1-8429-8962c3ca4479"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Evaluate accuracy of Gradient-boosted."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "c8873d6e-026e-4eac-892f-eb3a04a381ff"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gbt_accuracy = evaluator.evaluate(gbt_prediction)\n",
        "print(\"Accuracy of Gradient-boosted tree classifie is = %g\"% (gbt_accuracy))\n",
        "print(\"Test Error of Gradient-boosted tree classifie %g\"% (1.0 - gbt_accuracy))"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "e68a1020-cc72-4e3b-b991-91e8e9443b67"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Evaluating accuracy of DecisionTreeClassifier."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "68adac4e-78a1-4335-a1bc-416573fc2579"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dt_accuracy = evaluator.evaluate(dt_prediction)\n",
        "print(\"Accuracy of DecisionTreeClassifier is = %g\"% (dt_accuracy))\n",
        "print(\"Test Error of DecisionTreeClassifier = %g \" % (1.0 - dt_accuracy))"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "fc8e38d7-afae-4cdd-bbb5-1a73f4768e62"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### NaiveBayes"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "f428e414-b214-4cef-8451-86f0097cb773"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import NaiveBayes\n",
        "nb = NaiveBayes(labelCol=\"Survived\", featuresCol=\"features\")\n",
        "nb_model = nb.fit(trainingData)\n",
        "nb_prediction = nb_model.transform(testData)\n",
        "nb_prediction.select(\"prediction\", \"Survived\", \"features\").show()\n"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "cd7b7f7d-0b78-4ab4-ae2e-91e3489e6e89"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Evaluating accuracy of NaiveBayes."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "44e5a9eb-6339-458d-8a8e-6d407bbc138a"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nb_accuracy = evaluator.evaluate(nb_prediction)\n",
        "print(\"Accuracy of NaiveBayes is  = %g\"% (nb_accuracy))\n",
        "print(\"Test Error of NaiveBayes  = %g \" % (1.0 - nb_accuracy))"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "7fc980b5-de4c-4001-98ed-0d1f3484433e"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Support Vector Machine"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "697145ea-3804-4d6f-9109-f5c37ba100f3"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import LinearSVC\n",
        "svm = LinearSVC(labelCol=\"Survived\", featuresCol=\"features\")\n",
        "svm_model = svm.fit(trainingData)\n",
        "svm_prediction = svm_model.transform(testData)\n",
        "svm_prediction.select(\"prediction\", \"Survived\", \"features\").show()\n"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "a435b6ff-4382-4934-bee4-14ef1696f786"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Evaluating the accuracy of Support Vector Machine."
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "7a667979-14c4-4308-9f99-e3475dcef039"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "svm_accuracy = evaluator.evaluate(svm_prediction)\n",
        "print(\"Accuracy of Support Vector Machine is = %g\"% (svm_accuracy))\n",
        "print(\"Test Error of Support Vector Machine = %g \" % (1.0 - svm_accuracy))"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "6e89e1d3-c899-4f33-892f-3fe6b3bc5ca2"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "How to increase accuracy of a model ?\n",
        "  * Add new features or drop existing features and train model.\n",
        "  * Tune ML algorithm (https://spark.apache.org/docs/latest/ml-tuning.html)"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "ded78dc6-b9c0-4ba0-ab87-27a857048c4b"
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "python"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}